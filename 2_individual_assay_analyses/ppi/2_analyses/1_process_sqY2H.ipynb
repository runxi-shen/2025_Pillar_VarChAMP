{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the sqY2H data\n",
    "\n",
    "This is the main PPI profiling assay. This script loads the raw scores and processes them into consensus scores. Throughout the script, \"ad\" means \"activation domain\" and \"db\" means \"DNA binding domain\" of a transcription factor. A protein is fused to each of these; if the two proteins interact, then the TF is able to up-regulate the expression of a gene that allows the yeast to survive on the media. Thus, the yeast growth score is correlated to the protein-protein interaction strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir = \"../1_inputs/sqY2H\"\n",
    "outputs_dir = \"../3_outputs\"\n",
    "meta_outputs = \"../../../1_allele_collection/3_outputs\"\n",
    "meta_inputs = \"../../../1_allele_collection/1_inputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a list of ORF IDs and swim seq validation results\n",
    "raw_scores = pd.read_table(f\"{inputs_dir}/VarChAMP_1percent_Y2H_scores_v2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_plates = raw_scores[\"large_plate_name\"].unique()\n",
    "raw_scores = raw_scores[[\n",
    "    \"growth_score_id\", \"scoring_pla\", \"scoring_pos\", \"manual_score_growth\"\n",
    "]]\n",
    "raw_scores[['retest_batch', 'condition', 'retest_pla']] = raw_scores['scoring_pla'].str.extract(r\"([A-Z0-9]+)(r\\d+)_0*(\\d+)\")\n",
    "raw_scores['retest_pla'] = raw_scores['retest_pla'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control positions\n",
    "ctrl_wells = ['A01', 'A02', 'B01', 'B02',\n",
    "              'C01', 'C02', 'D01', 'D02',\n",
    "              'E01', 'E02', 'F01', 'F02',\n",
    "              'G01', 'G02', 'H01', 'H02',\n",
    "              'I01', 'I02', 'J01', 'J02',\n",
    "              'K01', 'K02', 'L01', 'L02']\n",
    "\n",
    "# Get first plates\n",
    "first_plates = np.unique([x.split(\"_\")[1] for x in large_plates])\n",
    "first_plates = first_plates.astype(int).tolist()\n",
    "\n",
    "# Add column indicating if its a media control well\n",
    "raw_scores['media_control'] = raw_scores['retest_pla'].isin(first_plates) & raw_scores['scoring_pos'].isin(ctrl_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scores match an allele to the 96 wells of the source plate \n",
    "allele_map = pd.read_csv(f\"{inputs_dir}/cp_files/mapping_y2h.csv\").drop(columns=['ad_symbol', 'db_symbol'])\n",
    "\n",
    "# create dictionary mapping 96 well position to 384 well position\n",
    "with open(f\"{inputs_dir}/mapping_384_to_96.rtf\", \"r\") as f:\n",
    "    rtf_content = f.read()\n",
    "\n",
    "plain_text = rtf_to_text(rtf_content)\n",
    "match = re.search(r\"mapping_384_to_96_d\\s*=\\s*({.*})\", plain_text, re.DOTALL)\n",
    "dict_str = match.group(1)\n",
    "dict_str = dict_str.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "map_384_96 = eval(dict_str)\n",
    "map_df = pd.DataFrame(list(map_384_96.items()), columns=['scoring_pos', 'retest_pos'])\n",
    "\n",
    "# Map the 96-pos to 384-pos\n",
    "allele_map = allele_map.merge(map_df, on=\"retest_pos\")\n",
    "allele_map['retest_pla'] = allele_map['retest_pla'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df = pd.DataFrame({\n",
    "    \"condition\":  [\"r07\", \"r08\", \"r09\", \"r10\", \"r11\", \"r12\", \"r13\", \"r14\", \"r15\", \"r16\",\"r17\", \"r18\"],\n",
    "    \"condition_name\": [\"LW\", \"LWA\", \"LWH1\", \"LWH10\", \"LWH25\", \"LWAH1\", \"LW\", \"LWA\", \"LWH1\", \"LWH10\", \"LWH25\", \"LWAH1\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add orf ids to scores\n",
    "scores = raw_scores.merge(allele_map, on=[\"scoring_pos\", \"retest_batch\", \"retest_pla\"], how=\"left\")\n",
    "scores['db_orf_id'] = scores['db_orf_id'].astype(str)\n",
    "scores['db_mut_id'] = scores['db_mut_id'].astype(str)\n",
    "scores['ad_orf_id'] = scores['ad_orf_id'].astype(str)\n",
    "scores['ad_mut_id'] = scores['ad_mut_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# give media controls their own IDs\n",
    "mask = scores['media_control'] == True\n",
    "scores.loc[mask, 'db_orf_id'] = (\n",
    "    scores.loc[mask, 'retest_pos'].astype(str) + '_' + scores.loc[mask, 'retest_pla'].astype(str)\n",
    ")\n",
    "scores.loc[mask, 'ad_orf_id'] = (\n",
    "    scores.loc[mask, 'retest_pos'].astype(str) + '_' + scores.loc[mask, 'retest_pla'].astype(str)\n",
    ")\n",
    "\n",
    "# Filter out r01-r06 and convert to other condition names\n",
    "exclude_list = [\"r01\", \"r02\", \"r03\", \"r04\", \"r05\", \"r06\"]\n",
    "scores = scores[~scores['condition'].isin(exclude_list)]\n",
    "\n",
    "# get only plates from relevant batches\n",
    "scores = scores[scores.standard_batch.isin([\"VUSAPWT1B1\",\"VUSAPWT1B2\",\"VUSAPWT2B1\",\"VUSAPWT6B1\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both db_orf_id and ad_orf_id must have values (otherwise they are empty wells)\n",
    "scores = scores[\n",
    "    (~scores.db_orf_id.isin(['nan', 'None'])) &\n",
    "    (~scores.ad_orf_id.isin(['nan', 'None']))\n",
    "]\n",
    "\n",
    "# add named condition column\n",
    "scores = scores.merge(condition_df, on=\"condition\")\n",
    "\n",
    "# write out file\n",
    "scores.to_csv(f\"{outputs_dir}/sqY2H/1_raw_individual_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most frequent score (consensus_score_list)\n",
    "scores = pl.DataFrame(scores)\n",
    "consensus_scores = scores.group_by(['ad_orf_id', 'ad_mut_id', 'db_orf_id', 'db_mut_id', 'condition_name', 'retest_batch', 'media_control']).agg(\n",
    "    pl.col(\"manual_score_growth\").mode().alias(\"consensus_score_list\")\n",
    ")\n",
    "\n",
    "# Determine whether there was a tie for the most frequent score\n",
    "consensus_scores = consensus_scores.with_columns(\n",
    "    pl.col(\"consensus_score_list\").list.len().alias(\"num_most_freq\")\n",
    ")\n",
    "\n",
    "# Easy cases - clear most frequent (num_most_freq == 1) or clear disagreement (num_most_freq > 2)\n",
    "consensus_easy = consensus_scores.filter(pl.col(\"num_most_freq\") != 2)\n",
    "consensus_easy = consensus_easy.with_columns(\n",
    "    pl.when(pl.col(\"num_most_freq\") == 1)\n",
    "      .then(pl.col(\"consensus_score_list\").list.first())\n",
    "      .when(pl.col(\"num_most_freq\") > 2)\n",
    "      .then(pl.lit(99999))\n",
    "      .otherwise(None)\n",
    "      .alias(\"consensus_score\")\n",
    ")\n",
    "\n",
    "# Hard cases - two scores were most frequent. Rules:\n",
    "# If they are consecutive (score_diff == 1), then take the maximum score\n",
    "# If they aren't, mark as failure (99999)\n",
    "# If one value is NaN, use the other value\n",
    "consensus_hard = consensus_scores.filter(pl.col(\"num_most_freq\") == 2)\n",
    "consensus_hard = consensus_hard.with_columns(\n",
    "    (pl.col(\"consensus_score_list\").list.get(0) - pl.col(\"consensus_score_list\").list.get(1))\n",
    "    .abs()\n",
    "    .alias(\"score_diff\")\n",
    ")\n",
    "consensus_hard = consensus_hard.with_columns(\n",
    "    pl.when(pl.col(\"score_diff\") > 1)\n",
    "      .then(pl.lit(99999))\n",
    "      .when(pl.col(\"score_diff\").is_null())\n",
    "      .then(\n",
    "          pl.col(\"consensus_score_list\")\n",
    "            .list.eval(pl.element().drop_nulls())\n",
    "            .list.first()\n",
    "      )\n",
    "      .when(pl.col(\"score_diff\") == 1)\n",
    "      .then(\n",
    "          pl.col(\"consensus_score_list\")\n",
    "            .list.eval(pl.element().max())\n",
    "            .list.first()\n",
    "      )\n",
    "      .otherwise(None)\n",
    "      .alias(\"consensus_score\")\n",
    ").drop(\"score_diff\")\n",
    "\n",
    "# Concatenate together\n",
    "consensus_scores = pl.concat([consensus_easy, consensus_hard]).drop([\n",
    "    \"consensus_score_list\", \"num_most_freq\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ad_orf_id to ad_symbol\n",
    "\n",
    "mapping_file = pd.read_csv(f\"{inputs_dir}/cp_files/mapping_y2h.csv\")\n",
    "mapping_file.dropna(subset=['ad_orf_id'], inplace=True)\n",
    "mapping_dict = mapping_file.set_index('ad_orf_id')['ad_symbol'].to_dict()\n",
    "\n",
    "consensus_scores = consensus_scores.with_columns(\n",
    "    pl.col(\"ad_orf_id\").replace_strict(mapping_dict, default=None).alias(\"ad_symbol\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with metadata and write out table\n",
    "metadata = pl.read_csv(f\"{meta_outputs}/slim_metadata.csv\").rename({\n",
    "    \"orf_id\": \"db_orf_id\",\n",
    "    \"mut_id\": \"db_mut_id\"\n",
    "}).with_columns(\n",
    "    pl.col(\"db_orf_id\").cast(pl.Float64).cast(pl.String).alias(\"db_orf_id\"),\n",
    "    pl.col(\"db_mut_id\").cast(pl.Float64).cast(pl.String).alias(\"db_mut_id\")\n",
    ")\n",
    "\n",
    "consensus_scores = consensus_scores.join(metadata, on=[\"db_orf_id\", \"db_mut_id\"], how=\"left\")\n",
    "consensus_scores.write_csv(f\"{outputs_dir}/sqY2H/2_raw_consensus_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Media conditions\n",
    "\n",
    "# 1 - Cells should either be all there or all not. Control media - are the two proteins present\n",
    "# 3 - 5, 2: strength of the interaction from low to high. 2 condition has a different media where it's harder for yeast to grow.\n",
    "# 6 - combination of 1 & 2 medias, but very complicated to logically process so currently not used but retained in case we use it in the future\n",
    "\n",
    "# 16 possible highest score: 4/4 for each of 3, 4, 5, 2, then divide by 16\n",
    "\n",
    "# dbX, empty abY. If yeast can still grow, it's called autoactivation\n",
    "# DB-X+0AD, DB-X+AD-Y. We get score for both. If both equal 4, then we discard. If 0 and 4, worked well. \n",
    "# If somewhere in between (ie. 2 and 4), we sometimes filter out (if the same value), sometimes keep with a score adjustment (check Georges code)\n",
    "\n",
    "# Postitions with both empty ad and db orf ids are media controls. \n",
    "# 6*96 well plate = 6*4 positions on 384, 6*384 on mega plate, but ctrls only spotted once per mega plate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
