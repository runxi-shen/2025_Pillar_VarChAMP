{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute abundance change for each allele\n",
    "\n",
    "This script takes in the single-cell FACS data as input, applies various filtering criteria, and then produces a z-score for the abundance of each allele relative to the reference on the same plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "import process_dualipa as proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dualipa_inputs = \"../1_inputs\"\n",
    "dualipa_outputs = \"../3_outputs\"\n",
    "meta_outputs = \"../../../1_allele_collection/3_outputs\"\n",
    "\n",
    "n_cell_threshold = 800\n",
    "wt_gfp_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in single-cell measurements\n",
    "pDEST_DUAL_df = pd.read_parquet(f\"{dualipa_outputs}/facs_single_cell.parquet\")\n",
    "\n",
    "# Filter to only keep wells with more than 800 cells\n",
    "pDEST_DUAL_df = pDEST_DUAL_df[pDEST_DUAL_df['n_cells'] > n_cell_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean and median measurements per well\n",
    "keep_cols = ['symbol', 'node_type', 'nt_change','aa_change', 'pla', 'well', 'coordinates', 'n_cells', 'orf_id', 'mut_id', 'valid_well']\n",
    "pDEST_DUAL_avg_df = pDEST_DUAL_df[keep_cols + ['avg_gfp', 'avg_mcherry','avg_GFP_mCherry_ratio']].drop_duplicates()\n",
    "pDEST_DUAL_median_df = pDEST_DUAL_df[keep_cols + ['median_gfp', 'median_mcherry','median_GFP_mCherry_ratio']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute the mean and mediam scores\n",
    "\n",
    "__Functions for computing is in ```process_dualipa.py```__\n",
    "\n",
    "Maxime's notes:\n",
    "\n",
    "    Instead of using Georges' approach to compute the assay's variability, which uses a step with random pairings, \n",
    "    Luke suggested to compute a STD from the Log2FC of all individual WT measurements, relative to the mean of the WT of each gene.\n",
    "    This is a more robust approach, as it does not rely on random pairings.\n",
    "    This function computes the STD of the Log2FC of all individual WT measurements, relative to the mean of the WT of each gene.\n",
    "    Returns a tuple with the mean and the STD of the Log2FC of all individual WT measurements, relative to the mean of the WT of each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _d objects: mean or median GFP_mCherry ratio of each unique (wt orf, plate) combination\n",
    "# _l objects: list of WT:WT abundances (GFP:mCherry ratios) to estimate assay variability\n",
    "wt_avg_d, wt_ratio_l = proc.get_wt_variability_d(pDEST_DUAL_avg_df, pDEST_DUAL_df)\n",
    "wt_median_d, wt_ratio_l_median = proc.get_wt_variability_d_median(pDEST_DUAL_median_df, pDEST_DUAL_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_std, wt_mean = proc.wt_log2fc_variability(pDEST_DUAL_avg_df, pDEST_DUAL_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be two WT replicates on same plate to compute the ratio, otherwise we return a NaN\n",
    "pDEST_DUAL_avg_allele_df = proc.get_pDEST_DUAL_avg_allele_df(pDEST_DUAL_avg_df, wt_avg_d, wt_ratio_l, wt_std, wt_mean)\n",
    "pDEST_DUAL_median_allele_df = proc.get_pDEST_DUAL_median_allele_df(pDEST_DUAL_median_df, wt_median_d, wt_ratio_l_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append metadata and write out files\n",
    "metadata = pl.read_csv(f\"{meta_outputs}/slim_metadata.csv\").with_columns(\n",
    "    pl.col(\"orf_id\").cast(pl.Float64).alias(\"orf_id\"),\n",
    "    pl.col(\"mut_id\").cast(pl.Float64).alias(\"mut_id\")\n",
    ")\n",
    "\n",
    "mean_df = pl.DataFrame(pDEST_DUAL_avg_allele_df)\n",
    "median_df = pl.DataFrame(pDEST_DUAL_median_allele_df)\n",
    "\n",
    "mean_df = mean_df.join(metadata, on=[\"orf_id\", \"mut_id\"], how=\"left\")\n",
    "median_df = median_df.join(metadata, on=[\"orf_id\", \"mut_id\"], how=\"left\")\n",
    "\n",
    "mean_df.write_csv(f'{dualipa_outputs}/DUALIPA_mean_zscore.csv')\n",
    "median_df.write_csv(f'{dualipa_outputs}/DUALIPA_median_zscore.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varchamp",
   "language": "python",
   "name": "varchamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
